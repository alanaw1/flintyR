<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Exchangeability and Homogeneity ‚Ä¢ flintyR</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Exchangeability and Homogeneity">
<meta property="og:description" content="flintyR">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">flintyR</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/ex_vs_hom.html">Exchangeability and Homogeneity</a>
    </li>
    <li>
      <a href="../articles/extras.html">Extras</a>
    </li>
    <li>
      <a href="../articles/intro.html">Introduction</a>
    </li>
    <li>
      <a href="../articles/single-cell-atac-seq.html">Analysis of scATAC-seq Data</a>
    </li>
    <li>
      <a href="../articles/wvs.html">Analysis of World Values Survey Data</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="ex_vs_hom_files/header-attrs-2.7/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Exchangeability and Homogeneity</h1>
            
      
      
      <div class="hidden name"><code>ex_vs_hom.Rmd</code></div>

    </div>

    
    
<p>How many ways can a multivariate dataset be heterogeneous? What has exchangeability got to do with it?</p>
<p>üìñ In this vignette, we will</p>
<ul>
<li>explore different types of heterogeneities, connecting them to a ‚Äúworking definition‚Äù of <em>statistical homogeneity</em>,</li>
<li>explain how ‚Äústatistical homogeneity‚Äù is better replaced by exchangeability (when trying to detect subpopulations in a given sample)</li>
<li>investigate, using <strong>flinty</strong>, the tension between sample exchangeability and feature independence</li>
</ul>
<p>The punchline is that non-exchangeability is a mathematically concrete way to capture our intuition of existence of subpopulations in a given sample, and there are types of statistical heterogeneities that still give rise to exchangeable data. Thus, users of our test should take care in interpreting results obtained from our test, or any other test of structured-ness of data (e.g., the <a href="https://rdrr.io/bioc/LEA/man/main_tracyWidom.html">Tracy-Widom test</a> of largest singular value).</p>
<div id="a-working-definition-of-statistical-homogeneity" class="section level1">
<h1 class="hasAnchor">
<a href="#a-working-definition-of-statistical-homogeneity" class="anchor"></a>A ‚ÄúWorking Definition‚Äù of Statistical Homogeneity</h1>
<p>Scientists frequently think about homogeneous populations, homogeneous proportions, and homogeneous samples. We run tests of homogeneity on labeled samples (e.g., the <span class="math inline">\(\chi^2\)</span> test). With labeled samples, one could directly formalize homogeneity as ‚Äúlittle to no difference with respect to labels.‚Äù Indeed, the <span class="math inline">\(\chi^2\)</span> test of homogeneity is a test of whether categorical proportions are similar enough between distinctly labeled samples. So are many other tests of homogeneity defined (e.g., <a href="https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/cochvari.htm">Cochran‚Äôs test</a> for homogeneity of variances between <span class="math inline">\(k\)</span> groups, the <a href="https://onlinelibrary.wiley.com/doi/10.1002/9781118445112.stat01756">Potthoff-Whittinghill test</a> of homogeneity between two classes). The general test of homogeneity is typically a ‚Äútest of homogeneity of quantity <span class="math inline">\(x\)</span> with respect to various groups of samples.‚Äù</p>
<p>Defining homogeneity without sample labels is less straightforward. Here are some approaches taken by others:</p>
<ul>
<li>In Volume 5 of the <em>Encyclopedia of Statistical Sciences</em> (second edition <a href="https://www.wiley.com/en-gb/Encyclopedia+of+Statistical+Sciences%2C+16+Volume+Set%2C+2nd+Edition-p-9780471150442">published</a> in 2006 by Wiley), on p.¬†3207 the entry ‚ÄúHomogeneity and Tests of Homogeneity‚Äù begins with the sentence, ‚Äúhomogeneity refers to sameness or similarity.‚Äù The rest of the paragraph, and indeed the remainder of the entry, frames homogeneity with respect to a collection of populations.</li>
<li>The environmental health book <em>Radiation-Induced Processes of Adaption: Research by Statistical Modeling</em> (<a href="https://www.springer.com/gp/book/9789400766297">published</a> in 2013 by Springer) writes in their Glossary, on p.¬†173, that homogeneity ‚Äúrelate[s] to the validity of the often convenient assumption that the statistical properties of any one part of an overall dataset are the same as any other part.‚Äù</li>
</ul>
<p>The statistics encyclopedia does not provide any definition of homogeneity of a dataset without labels. This reflects our point in the beginning about how many tests of homogeneity typically require sample labels. On the other hand, the environmental health book describes a formalizable description of homogeneity, namely the clause about statistical properties of any one part looking the same as another.</p>
<p>Mathematically, the description above says that given a dataset <span class="math inline">\(\{\mathbf{x}_1,\ldots,\mathbf{x}_N\} \subset \mathbb{R}^P\)</span>, for it to be homogeneous it should satisfy the following property: for any pair of disjoint subsamples <span class="math inline">\(\mathcal{S}=\{\mathbf{x}_i\}\)</span> and <span class="math inline">\(\mathcal{S}'=\{\mathbf{x}_{i'}\}\)</span>, the distributions of the pair of subsamples <span class="math inline">\(F_\mathcal{S}\)</span> and <span class="math inline">\(F_{\mathcal{S}'}\)</span> should not differ by too much: <span class="math display">\[\begin{equation}
d(F_\mathcal{S},F_{\mathcal{S}'}) \approx 0,\label{eq:1}
\end{equation}\]</span> where <span class="math inline">\(d(\cdot,\cdot)\)</span> is some measure of distance between distributions (e.g., Wasserstein distance or total variation distance).</p>
<p>Suppose we adopt the description above as a working definition of statistical homogeneity. We shall explain why this definition is problematic.</p>
<div id="a-single-population-doesnt-imply-statistical-homogeneity" class="section level2">
<h2 class="hasAnchor">
<a href="#a-single-population-doesnt-imply-statistical-homogeneity" class="anchor"></a>A Single Population Doesn‚Äôt Imply ‚ÄúStatistical Homogeneity‚Äù</h2>
<p>Let us define a population, for which there are <span class="math inline">\(P\)</span> observable features. These features are statistically independent and identically distributed according to the mixture distribution <span class="math inline">\(\frac{1}{2}N(-2,1) + \frac{1}{2}N(2,1)\)</span>. This means that we flip a fair coin to decide whether a feature is drawn from <span class="math inline">\(N(-2,1)\)</span> or from <span class="math inline">\(N(2,1)\)</span>.</p>
<p>For illustrative purposes, let‚Äôs assume that <span class="math inline">\(P=2\)</span> and draw <span class="math inline">\(N=40\)</span> samples from this distribution.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Helper function to draw samples</span>
<span class="va">drawSamples</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">N</span><span class="op">)</span>  <span class="op">{</span>
  <span class="co"># create sequence to return</span>
  <span class="va">to_return</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span>
  
  <span class="co"># grow the sequence</span>
  <span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span>
    <span class="co"># flip a fair coin</span>
    <span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">0.5</span><span class="op">)</span> 
    
    <span class="co"># if heads...</span>
    <span class="kw">if</span> <span class="op">(</span><span class="va">p</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span>
      <span class="va">to_return</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">to_return</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, mean <span class="op">=</span> <span class="fl">2</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> 
    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span>
      <span class="co"># if tails...</span>
      <span class="va">to_return</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">to_return</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, mean <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>, sd <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
    <span class="op">}</span>
  <span class="op">}</span>
  
  <span class="co"># return</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">to_return</span><span class="op">)</span>
<span class="op">}</span>

<span class="co">## Generate array</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2021</span><span class="op">)</span>
<span class="va">ex_array</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fu">drawSamples</span><span class="op">(</span><span class="fl">40</span><span class="op">)</span><span class="op">)</span>

<span class="co">## Annotate positive and negative labels</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="va">ex_array</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">ex_array</span><span class="op">)</span>
<span class="va">ex_array</span><span class="op">$</span><span class="va">POSLAB</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="op">-</span><span class="fl">1</span> <span class="op">*</span> <span class="op">(</span><span class="va">ex_array</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span> <span class="op">&lt;</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">ex_array</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">&lt;</span> <span class="fl">0</span><span class="op">)</span> <span class="op">+</span>
                      <span class="op">(</span><span class="va">ex_array</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span> <span class="op">&amp;</span> <span class="va">ex_array</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span> 

<span class="va">ex_array</span> <span class="op">&lt;-</span> <span class="va">ex_array</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">ex_array</span><span class="op">$</span><span class="va">POSLAB</span><span class="op">)</span>,<span class="op">]</span>

<span class="co">## Print</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ex_array</span><span class="op">)</span>
<span class="co">#&gt;             V1         V2 POSLAB</span>
<span class="co">#&gt; 6  -1.98622806 -2.2519641     -1</span>
<span class="co">#&gt; 11 -0.42206811 -3.7976124     -1</span>
<span class="co">#&gt; 15 -0.27976384 -1.8282297     -1</span>
<span class="co">#&gt; 17 -1.07187288 -2.1035184     -1</span>
<span class="co">#&gt; 19 -2.88744463 -2.8259549     -1</span>
<span class="co">#&gt; 20 -0.89933137 -1.5167392     -1</span>
<span class="co">#&gt; 24 -3.58347390 -2.1220018     -1</span>
<span class="co">#&gt; 25 -0.46670339 -1.7728245     -1</span>
<span class="co">#&gt; 28 -0.23978627 -1.5496600     -1</span>
<span class="co">#&gt; 29 -2.78119689 -2.6229225     -1</span>
<span class="co">#&gt; 35 -0.80789690 -1.2874858     -1</span>
<span class="co">#&gt; 36 -2.36881809 -1.7987080     -1</span>
<span class="co">#&gt; 37 -1.66402782 -1.8842061     -1</span>
<span class="co">#&gt; 38 -1.57271087 -1.5292104     -1</span>
<span class="co">#&gt; 1  -1.21497757  1.3543091      0</span>
<span class="co">#&gt; 2  -1.65135050  2.3919333      0</span>
<span class="co">#&gt; 3   1.37711402 -0.7146051      0</span>
<span class="co">#&gt; 4   0.07743048 -2.9836134      0</span>
<span class="co">#&gt; 7   2.11479879 -2.2323998      0</span>
<span class="co">#&gt; 9   3.35407495 -2.1500867      0</span>
<span class="co">#&gt; 14 -2.94244327  1.6624908      0</span>
<span class="co">#&gt; 21 -1.94058234  1.2836487      0</span>
<span class="co">#&gt; 22 -0.98055658  1.9465417      0</span>
<span class="co">#&gt; 23 -1.53559759  2.5631673      0</span>
<span class="co">#&gt; 26 -2.08707112  1.4912997      0</span>
<span class="co">#&gt; 27  4.59310821 -2.0767792      0</span>
<span class="co">#&gt; 30  1.96562251 -3.1957732      0</span>
<span class="co">#&gt; 31 -1.53005963  1.4059371      0</span>
<span class="co">#&gt; 32  1.27444279 -4.7007150      0</span>
<span class="co">#&gt; 33  2.60824840 -3.4950350      0</span>
<span class="co">#&gt; 39 -2.07556434  1.5973352      0</span>
<span class="co">#&gt; 40 -3.50559994  0.1976965      0</span>
<span class="co">#&gt; 5   2.16989432  2.1221998      1</span>
<span class="co">#&gt; 8   1.72717482  1.9570100      1</span>
<span class="co">#&gt; 10  3.60447011  0.9606129      1</span>
<span class="co">#&gt; 12  2.13138902  0.6074574      1</span>
<span class="co">#&gt; 13  1.29390807  1.2390690      1</span>
<span class="co">#&gt; 16  3.20811525  2.6332890      1</span>
<span class="co">#&gt; 18  0.54455665  2.5339593      1</span>
<span class="co">#&gt; 34  1.49970920  2.4869767      1</span></code></pre></div>
<p>Notice we have added positive labels in the third column to annotate whether both features are positive or negative. The example shows that this sample forms two clusters of negative and positive observation pairs, <span class="math inline">\(\mathcal{S}_-\)</span> and <span class="math inline">\(\mathcal{S}_+\)</span>. Clearly these disjoint subsamples are far apart, thus rendering the dataset itself not statistically homogeneous under the working definition. See the 2D plot below for a visualization.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Plot 2D </span>
<span class="va">ratio</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">ex_array</span>, <span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">V1</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">V2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="co"># compute ratio to fix make plot square</span>

<span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">ex_array</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">V1</span>, y <span class="op">=</span> <span class="va">V2</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">POSLAB</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">coord_fixed</span><span class="op">(</span>ratio <span class="op">=</span> <span class="va">ratio</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">theme_bw</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">labs</span><span class="op">(</span>colour <span class="op">=</span> <span class="st">"POSLAB"</span><span class="op">)</span></code></pre></div>
<p><img src="ex_vs_hom_files/figure-html/example_2-1.png" width="700"></p>
<p>This observation is true in a more general setting. For <span class="math inline">\(P\)</span> features independently distributed according to the mixture distribution described above, it is always possible to draw a finite sample consisting of <span class="math inline">\(N\)</span> observations, which contains two disjoint subsamples that are far apart. Here is some mathematical intuition. For each observation <span class="math inline">\(\mathbf{x}_n\)</span>, let <span class="math inline">\(\mathbf{1}\{\mathbf{x}_n &gt; \mathbf{0}\}\)</span> and <span class="math inline">\(\mathbf{1}\{\mathbf{x}_n &lt; \mathbf{0}\}\)</span> denote, respectively, the event that it is positive and the event that it is negative. A positive observation <span class="math inline">\(\mathbf{x}_n=(x_{n1},\ldots,x_{nP})\)</span> means that each element <span class="math inline">\(x_{nj}\)</span> is positive; likewise for a negative observation. Each component of <span class="math inline">\(\mathbf{x}_n\)</span> is IID according to the mixture distribution that is symmetric about <span class="math inline">\(0\)</span>. This implies that each element <span class="math inline">\(x_{nj}\)</span> satisfies <span class="math inline">\(\mathbb{P}(x_{nj}&gt;0) =\mathbb{P}(x_{nj}&lt;0) = 0.5\)</span>. Multiplying these probabilities, we see that <span class="math inline">\(\mathbb{P}(\mathbf{x}_n &gt; \mathbf{0})=\mathbb{P}(\mathbf{x}_n &lt; \mathbf{0})=0.5^P\)</span>. The indicators are thus Bernoulli distributed with success probability <span class="math inline">\(0.5^P\)</span>. Let <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> denote the number of positive and negative observations contained in a random draw. Since our observations are independently drawn, we see that for any pair of natural numbers <span class="math inline">\((k_+,k_-)\)</span>, <span class="math display">\[\begin{equation*}
\mathbb{P}(K_+=k_+, K_-=k_-) = {N\choose k_+}{N-k_+ \choose k_-} \left(\frac{1}{2^P}\right)^{k_+ + k_-}.
\end{equation*}\]</span> In particular, there is a positive probability that the finite sample drawn at random contains two sets <span class="math inline">\(\mathcal{S}_+, \mathcal{S}_-\)</span> of sizes <span class="math inline">\(k_+\)</span> and <span class="math inline">\(k_-\)</span>. These two sets containing positive observations and negative observations exclusively, we see that they are necessarily far apart.</p>
<p>In fact, it can be shown that for large enough sample sizes, with high probability a finite sample drawn will contain two reasonably large disjoint subsamples <span class="math inline">\(\mathcal{S}_+\)</span> and <span class="math inline">\(\mathcal{S}_-\)</span> that are far apart (i.e., with total variation distance <span class="math inline">\(d_{\text{TV}}(F_{\mathcal{S}_+},F_{\mathcal{S}_-}) =1\)</span>).</p>
<details><summary><b>I like maths. Show me a proof.</b>
</summary>
Fantastic. Observe that <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span>, defined above, can be written as <span class="math inline">\(K_+=\sum_{n=1}^N \mathbf{1}\{\mathbf{x}_n &gt; \mathbf{0}\}\)</span> and <span class="math inline">\(K_-=\sum_{n=1}^N \mathbf{1}\{\mathbf{x}_n &lt; \mathbf{0}\}\)</span>. We already saw that the indicator events <span class="math inline">\(\mathbf{1}\{\cdot\}\)</span> are Bernoulli distributed with success probability <span class="math inline">\(0.5^P\)</span>. Thus <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> are both <span class="math inline">\(\text{Bin}(N,0.5^P)\)</span> distributed.<br> Bernoulli distributions are subGaussian with subGaussian parameter <span class="math inline">\(\sigma^2=0.25\)</span>, regardless of the success probability <span class="math inline">\(p\)</span>. Thus <span class="math inline">\(\text{Bin}(N,p)\)</span> distributions are subGaussian with subGaussian parameter <span class="math inline">\(\sigma^2=0.25N\)</span>. Being subGaussian implies that a distribution has nice tail bounds. More precisely, Chernoff‚Äôs bound (see Chapter 2 of <a href="https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E">this text</a> or the last page of <a href="https://math.mit.edu/~goemans/18310S15/chernoff-notes.pdf">this course note</a>) implies that <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> satisfy the following concentration inequalities: for any positive <span class="math inline">\(\epsilon\)</span>, <span class="math display">\[\begin{eqnarray*}
\mathbb{P}(|K_+ - \mathbb{E}[K_+]| &gt; \epsilon) &amp; \leqslant &amp; 2\exp\left(-\frac{2\epsilon^2}{N}\right), \\
\mathbb{P}(|K_- - \mathbb{E}[K_-]| &gt; \epsilon) &amp; \leqslant &amp; 2\exp\left(-\frac{2\epsilon^2}{N}\right).
\end{eqnarray*}\]</span> Let‚Äôs use these concentration inequalities to show that for a finite sample drawn, <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> will be close to their expectations with high probability. Then as <span class="math inline">\(N\)</span> becomes large enough, we will see that <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> become reasonably large.<br> First, by linearity of expectation we have <span class="math display">\[\begin{equation*}
\mathbb{E}[K_+] = \mathbb{E}[K_-] = \frac{N}{2^P}.
\end{equation*}\]</span> Next, by the union bound <span class="math display">\[\begin{eqnarray*}
\mathbb{P}(|K_+ - \mathbb{E}[K_+]| &gt; \epsilon \vee |K_- - \mathbb{E}[K_-]| &gt; \epsilon) &amp; \leqslant &amp; \mathbb{P}(|K_+ - \mathbb{E}[K_+]| &gt; \epsilon) + \mathbb{P}(|K_- - \mathbb{E}[K_-]| &gt; \epsilon) \\
&amp; \leqslant &amp; 4 \exp\left(-\frac{2\epsilon^2}{N}\right),
\end{eqnarray*}\]</span> which implies that the complementary event (the ‚Äúgood‚Äù event) satisfies <span class="math display">\[\begin{equation*}
\mathbb{P}(|K_+ - \mathbb{E}[K_+]| \leqslant \epsilon \wedge |K_- - \mathbb{E}[K_-]| \leqslant \epsilon) \geqslant 1 - 4 \exp\left(-\frac{2\epsilon^2}{N}\right).
\end{equation*}\]</span> Now set <span class="math inline">\(\epsilon = N^{0.55}\)</span> and let <span class="math inline">\(N=M \cdot 2^P\)</span> (so that <span class="math inline">\(N\)</span> is some multiple <span class="math inline">\(M\)</span> of <span class="math inline">\(2^P\)</span> where we shall choose <span class="math inline">\(M\)</span> large enough). The last inequality says that with probability at least <span class="math inline">\(1 - 4\exp\left(-2N^{0.1}\right)=1-4\exp\left(-2 \cdot 2^{0.1P} \cdot M^{0.1}\right)\)</span>, a random sample contains <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> positive and negative observations, with <span class="math display">\[\begin{equation*}
M - 2^{0.55P}\cdot M^{0.55} \leqslant K_+, K_- \leqslant M + 2^{0.55P}\cdot M^{0.55}.
\end{equation*}\]</span> This shows that <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> grow like <span class="math inline">\(M\)</span> with probability <span class="math inline">\(1-4\exp\left(-2 \cdot 2^{0.1P} \cdot M^{0.1}\right)\)</span>, a quantity that approaches <span class="math inline">\(1\)</span> as <span class="math inline">\(M\to\infty\)</span>. Thus, given fixed <span class="math inline">\(P\)</span>, for <span class="math inline">\(M\)</span> large enough (so that the sample size <span class="math inline">\(N\)</span> is also large), we have two reasonably large disjoint subsamples <span class="math inline">\(\mathcal{S}_+\)</span> and <span class="math inline">\(\mathcal{S}_-\)</span> that are far apart. <br> For perspective, set <span class="math inline">\(P=5\)</span> and <span class="math inline">\(M=100\)</span>. We draw <span class="math inline">\(N=M\times 2^P = 3200\)</span> observations at random from the population. The argument above says that with probability at least <span class="math inline">\(1-4 \exp\left(-2 \cdot 2^{0.1P} \cdot M^{0.1}\right) = 0.95\)</span>, the sample will have <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> positive and negative samples, with <span class="math inline">\(K_+\)</span> and <span class="math inline">\(K_-\)</span> lying between <span class="math inline">\(M - 2^{0.55P}\cdot M^{0.55}\approx 15.3\)</span> and <span class="math inline">\(M - 2^{0.55P}\cdot M^{0.55}\approx 184.7\)</span>.<br><br>
To finish the argument, we have to quantify ‚Äúfar apart‚Äù in terms of some distance computed between the two empirical distributions obtained from the positive and negative sets, <span class="math inline">\(\mathcal{S}_+\)</span> and <span class="math inline">\(\mathcal{S}_-\)</span>. We shall use the total variation distance, <span class="math display">\[\begin{equation*}
d_\text{TV}(F,F') = \sup\{|\mu_F(A) - \mu_{F'}(A)|: A \subseteq \mathbb{R}^P\}.
\end{equation*}\]</span> The total variation distance is the largest possible difference between the probabilities that the two probability distributions <span class="math inline">\(\mu_F\)</span> and <span class="math inline">\(\mu_F'\)</span> can assign to the same event. It ranges between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Now for <span class="math inline">\(\mathcal{S}_+\)</span> and <span class="math inline">\(\mathcal{S}_-\)</span>, the corresponding empirical distributions, denoted by <span class="math inline">\(\mu_+\)</span> and <span class="math inline">\(\mu_-\)</span>, are fully supported on <span class="math inline">\(\mathbb{R}_+^P\)</span> and <span class="math inline">\(\mathbb{R}_-^P\)</span> respectively. Thus the largest possible difference is at least the difference between the probabilities that <span class="math inline">\(\mu_+\)</span> and <span class="math inline">\(\mu_-\)</span> assign to the set of positive <span class="math inline">\(P\)</span>-dimensional reals, which is <span class="math inline">\(1\)</span>. Therefore <span class="math inline">\(d_{\text{TV}}(F_{\mathcal{S}_+},F_{\mathcal{S}_-}) \geqslant 1\)</span>, which explicitly quantifies that the subsamples are distributionally far apart. (In fact it is equal to <span class="math inline">\(1\)</span>, since <span class="math inline">\(d_\text{TV}\)</span> cannot exceed <span class="math inline">\(1\)</span> as was mentioned earlier.)
</details><p><br> The example above and its generalization show that even if our data originates from a single population, statistical heterogeneity may still arise in finite samples. They demonstrate the need for careful methods to decide whether a finite sample originated from a single population. (Granted, this single population contains <span class="math inline">\(2^P\)</span> clusters, but for large values of <span class="math inline">\(P\)</span> one needs exponentially larger values of <span class="math inline">\(N\)</span> to capture these clusters. This is why we often do not consider clustering a sample into <span class="math inline">\(2^P\)</span> roughly evenly-sized subsamples each containing only a few samples.)</p>
</div>
<div id="hypothesis-testing-and-limitations-of-parametric-approaches" class="section level2">
<h2 class="hasAnchor">
<a href="#hypothesis-testing-and-limitations-of-parametric-approaches" class="anchor"></a>Hypothesis Testing and Limitations of Parametric Approaches</h2>
<p>To decide whether a finite sample originated from a single population given that it was sampled at random, we must ask if patterns observed in the finite sample occur due to chance or to the presence of some signal.</p>
<p>If we have enough information about the population to describe its distribution reasonably accurately (e.g., as a mixture of Gaussians like in the example above), then we could compute the probability of observing the particular finite sample, or at least estimate this probability, say by estimating the distributional parameters using maximum likelihood estimators obtained from the finite sample. We can then reject or accept the null hypothesis that our finite sample was drawn from the particular distribution, at some specified significance level <span class="math inline">\(\alpha\)</span>. This is precisely what we are doing when we perform hypothesis testing.</p>
<p>Unfortunately, in many real scenarios we don‚Äôt have enough information. There could be</p>
<ul>
<li>A lack of reasonable justification for a model choice (e.g., why should we always expect a feature to be modeled well by Gaussians even if it has worked well so far on limited samples?)</li>
<li>Insufficient domain knowledge to assume a model (e.g., what kind of distribution should we use to model our data given that we have very little knowledge about its statistical properties?)</li>
<li>Complex dependencies preventing a clear model choice (e.g., how can we decide between multiple equally complex hierarchical models when they both seem to fit the data well?)</li>
</ul>
<p>On top of model-driven concerns, there are other technical difficulties.</p>
<ul>
<li>For models that have massive numbers of parameters rendering densities extremely small (risking numerical underflow issues), what is a reasonable low-dimensional test statistic based on which to compute the probability of observing our sample?</li>
<li>The more complex and expressive a model, the more difficult it is to identify or compute sufficient statistics, which are potential candidates for test statistics especially when estimating distributional parameters is intractable or noisy.</li>
</ul>
<p>Given the potential challenge of asserting a parametric model over our finite sample, we need strategies that avoid parametric modelling. One such strategy exploits the exchangeability of finite samples drawn from a population.</p>
</div>
</div>
<div id="exchangeability" class="section level1">
<h1 class="hasAnchor">
<a href="#exchangeability" class="anchor"></a>Exchangeability</h1>
<p>A finite sequence <span class="math inline">\((\mathbf{x}_1,\ldots,\mathbf{x}_N)\)</span> of random variables is called exchangeable if <span class="math display">\[\begin{equation*}
(\mathbf{x}_1,\ldots,\mathbf{x}_N) \overset{D}{=}(\mathbf{x}_{\pi(1)},\ldots,\mathbf{x}_{\pi(N)})
\end{equation*}\]</span> for each permutation <span class="math inline">\(\pi\)</span> of <span class="math inline">\(\{1,\ldots,N\}\)</span>. This definition is relevant to sampling from a population: a finite sample drawn via random sampling from a large population is exchangeable.</p>
<p>So how can exchangeability help us avoid parametric modelling while detecting the existence of subpopulations? Let us see a few examples of heterogeneous samples first, relating them to exchangeability.</p>
<div id="examples-of-exchangeable-and-heterogeneous-samples" class="section level2">
<h2 class="hasAnchor">
<a href="#examples-of-exchangeable-and-heterogeneous-samples" class="anchor"></a>Examples of Exchangeable and Heterogeneous Samples</h2>
<ol style="list-style-type: decimal">
<li>Mixture Distributions and Independent Features
<ul>
<li>We saw this in the example above.</li>
</ul>
</li>
<li>Mixture Distributions and Conditionally Independent Features (I)
<ul>
<li>Let each <span class="math inline">\(\mathbf{x}_n\overset{\text{iid}}{\sim} \frac{1}{2}F_1 + \frac{1}{2}F_2\)</span>, where the two source distributions contributing to the mixture are given by <span class="math inline">\(F_1 = \bigotimes_{i=1}^4 N(i^2, 1)\)</span> and <span class="math inline">\(F_2 =\bigotimes_{i=1}^4 N(1/i^2, 1)\)</span>. The <span class="math inline">\(\otimes\)</span> notation here means that the <span class="math inline">\(P=4\)</span> features are generated independently according to unit variance Gaussian distributions.<br>
</li>
</ul>
</li>
<li>Mixture Distributions and Conditionally Independent Features (II)
<ul>
<li>Let <span class="math inline">\(\mathbf{x}_n\overset{\text{iid}}{\sim} \frac{1}{2}F_1 + \frac{1}{2}F_2\)</span>, but now <span class="math inline">\(F_1 = N(1,1) \otimes N(4,1) \otimes N(-4,1) \otimes N(-1,1)\)</span> and <span class="math inline">\(F_2 = N(-1,1) \otimes N(-4,1) \otimes N(4,1)\otimes N(1,1)\)</span>.</li>
</ul>
</li>
</ol>
<p>Each of these examples generates heterogeneous finite samples, and their finite samples are exchangeable by definition. However, hopefully most readers will agree that only the last two examples represent the existence of subpopulations for which the joint distribution of their features cannot be modeled in a straightforward manner. Indeed, even though the first example generates samples containing <span class="math inline">\(2^P\)</span> clusters, we are able to mathematically describe its features as independent distributions. Additionally, comparing the last two examples reveals that distinct subpopulations can generate a variety of heterogeneities: Example 2 generates samples whose row sums differ by subpopulation, whereas Example 3 generates samples whose row sums look more homogeneous. We provide numerical examples below.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Helper functions to generate samples</span>
<span class="va">drawSamplesEx2</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">to_return</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">N</span>, <span class="op">{</span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">0.5</span><span class="op">)</span>
  <span class="kw">if</span> <span class="op">(</span><span class="va">p</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">4</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">4</span>,<span class="fl">9</span>,<span class="fl">16</span><span class="op">)</span><span class="op">)</span><span class="op">}</span> <span class="kw">else</span> <span class="op">{</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">4</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">/</span><span class="fl">4</span>,<span class="fl">1</span><span class="op">/</span><span class="fl">9</span>,<span class="fl">1</span><span class="op">/</span><span class="fl">16</span><span class="op">)</span><span class="op">)</span><span class="op">}</span>
  <span class="op">}</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">to_return</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">drawSamplesEx3</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">to_return</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">N</span>, <span class="op">{</span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">0.5</span><span class="op">)</span>
  <span class="kw">if</span> <span class="op">(</span><span class="va">p</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">4</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">4</span>,<span class="op">-</span><span class="fl">4</span>,<span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">}</span> <span class="kw">else</span> <span class="op">{</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">4</span>, mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>,<span class="op">-</span><span class="fl">4</span>,<span class="fl">4</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">}</span>
  <span class="op">}</span><span class="op">)</span>
  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">to_return</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span>

<span class="co">## Generate N = 40 observations for each example</span>
<span class="va">ex_2</span> <span class="op">&lt;-</span> <span class="fu">drawSamplesEx2</span><span class="op">(</span><span class="fl">40</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="op">)</span>
<span class="va">ex_2</span><span class="op">$</span><span class="va">ROWSUM</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">ex_2</span><span class="op">)</span>
<span class="va">ex_3</span> <span class="op">&lt;-</span> <span class="fu">drawSamplesEx3</span><span class="op">(</span><span class="fl">40</span><span class="op">)</span> <span class="op">%&gt;%</span> 
  <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="op">)</span>
<span class="va">ex_3</span><span class="op">$</span><span class="va">ROWSUM</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">ex_3</span><span class="op">)</span>

<span class="co">## Print</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ex_2</span><span class="op">)</span>
<span class="co">#&gt;             V1           V2          V3          V4      ROWSUM</span>
<span class="co">#&gt; 1   1.10068411  2.594680913  9.38260127 16.33230452 29.41027081</span>
<span class="co">#&gt; 2   1.48475660  1.343237643  0.41401952  1.07779944  4.31981320</span>
<span class="co">#&gt; 3   0.81377545  3.031421463  9.50078156 15.38272781 28.72870628</span>
<span class="co">#&gt; 4   1.51258354 -0.155761879  0.46730912 -0.26907087  1.55505991</span>
<span class="co">#&gt; 5  -0.98087962  4.027230179  9.71851167 15.46387139 28.22873362</span>
<span class="co">#&gt; 6   0.05052152  4.446261997  7.71117054 15.84219454 28.05014860</span>
<span class="co">#&gt; 7   0.59358747  5.008695586  9.48745936 15.05228171 30.14202412</span>
<span class="co">#&gt; 8  -0.25403793  1.021043169 -0.80024083 -0.63080537 -0.66404096</span>
<span class="co">#&gt; 9   0.53279877  0.319720620 -0.69743732 -0.03525892  0.11982315</span>
<span class="co">#&gt; 10  1.35541968  4.159138785  9.95539519 15.66035771 31.13031136</span>
<span class="co">#&gt; 11 -0.66652119  1.262734520 -1.50460850  1.85268546  0.94429029</span>
<span class="co">#&gt; 12  1.81878338  0.170349891 -0.37833832  0.91021875  2.52101369</span>
<span class="co">#&gt; 13  0.58194833  1.446715184 -0.14369037  0.45294984  2.33792298</span>
<span class="co">#&gt; 14  1.26115729 -0.742608643 -0.95226989  0.33678466 -0.09693658</span>
<span class="co">#&gt; 15  1.37110398  5.004709994  9.65418086 16.39887569 32.42887052</span>
<span class="co">#&gt; 16  1.27800265  4.147040753  7.80371062 16.09013627 29.31889029</span>
<span class="co">#&gt; 17  1.68299909  4.226607611  8.05642898 15.97404659 29.94008227</span>
<span class="co">#&gt; 18  1.51486548  0.011144777  0.69292968  0.33269527  2.55163521</span>
<span class="co">#&gt; 19 -0.19992547  0.067671152  0.03535128 -0.08128418 -0.17818723</span>
<span class="co">#&gt; 20  0.07594888  0.688599459  0.09901110 -0.58341074  0.28014870</span>
<span class="co">#&gt; 21  0.20860783  3.769535335 11.02650847 17.09418625 32.09883789</span>
<span class="co">#&gt; 22  1.10078634 -0.141752488 -0.29995823 -0.85108128 -0.19200566</span>
<span class="co">#&gt; 23  2.47470011  0.808318503 -0.92924023  0.08050841  2.43428679</span>
<span class="co">#&gt; 24  2.91735175  0.013429884  1.68273446  0.54335007  5.15686617</span>
<span class="co">#&gt; 25  0.42174710  3.981750716  9.23159574 16.86499219 30.50008575</span>
<span class="co">#&gt; 26  0.96871265  4.999043959  9.30031928 15.99480639 31.26288228</span>
<span class="co">#&gt; 27  1.39871518 -0.067974504 -0.30272071 -1.21942516 -0.19140520</span>
<span class="co">#&gt; 28 -0.98570884  1.064365867  1.20936250 -1.37819404 -0.09017451</span>
<span class="co">#&gt; 29  0.90460605  3.690439522 10.88048376 15.95194941 31.42747873</span>
<span class="co">#&gt; 30  0.56565786 -1.623007540 -0.69193341  0.39482209 -1.35446100</span>
<span class="co">#&gt; 31 -0.60581924  5.239595843  9.48406942 17.39158160 31.50942762</span>
<span class="co">#&gt; 32  0.07296961  1.255608690  0.02515366  1.00052486  2.35425682</span>
<span class="co">#&gt; 33  1.42840801  0.246676870 -1.77404647  2.89560466  2.79664307</span>
<span class="co">#&gt; 34  2.43082511  2.858982545  9.89600666 16.08283791 31.26865222</span>
<span class="co">#&gt; 35  0.02845901  3.917522671  6.34691454 17.65469664 27.94759286</span>
<span class="co">#&gt; 36  0.51629898  6.210854157  9.08082445 16.71610646 32.52408405</span>
<span class="co">#&gt; 37  2.63280169  3.370155625  9.60974536 15.34170932 30.95441200</span>
<span class="co">#&gt; 38 -0.56224282  2.012216050  7.50794616 16.25524142 25.21316081</span>
<span class="co">#&gt; 39 -0.41975773  0.005161406 -0.10744747 -1.31323943 -1.83528322</span>
<span class="co">#&gt; 40  1.18694529 -0.444774213 -0.34833190 -0.41250551 -0.01866634</span>
<span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Variance of row sums for ex_2 is "</span>, <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">ex_2</span><span class="op">$</span><span class="va">ROWSUM</span><span class="op">)</span>, <span class="st">"."</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
<span class="co">#&gt; Variance of row sums for ex_2 is 218.8347.</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">ex_3</span><span class="op">)</span>
<span class="co">#&gt;             V1        V2        V3          V4     ROWSUM</span>
<span class="co">#&gt; 1   3.04503083  3.934625 -4.513511  0.99013427  3.4562793</span>
<span class="co">#&gt; 2  -1.54091497 -4.673914  4.680151  1.18671480 -0.3479628</span>
<span class="co">#&gt; 3   0.94311489 -4.390760  4.471871  1.14283428  2.1670596</span>
<span class="co">#&gt; 4   0.99425697  2.958406 -4.325231 -1.60856295 -1.9811318</span>
<span class="co">#&gt; 5   0.12998607  2.537576 -4.338526 -3.04256186 -4.7135258</span>
<span class="co">#&gt; 6  -0.08558911  3.914713 -3.534498 -1.00002890 -0.7054034</span>
<span class="co">#&gt; 7   0.15327102  2.807977 -4.642664 -1.15934744 -2.8407635</span>
<span class="co">#&gt; 8  -2.80955074 -4.779455  2.011673  1.99723557 -3.5800973</span>
<span class="co">#&gt; 9  -1.13217201 -2.582155  2.875181 -0.29546824 -1.1346136</span>
<span class="co">#&gt; 10 -0.45467373 -5.366537  2.883952  1.16247234 -1.7747861</span>
<span class="co">#&gt; 11  0.94723075  1.910360 -5.568118 -0.24966052 -2.9601885</span>
<span class="co">#&gt; 12  0.08967148 -5.402433  4.935231  0.98800737  0.6104769</span>
<span class="co">#&gt; 13  0.10519644  3.679681 -2.082365 -4.04499331 -2.3424809</span>
<span class="co">#&gt; 14  2.13430170  4.510970 -5.497154 -2.11755526 -0.9694374</span>
<span class="co">#&gt; 15  1.47702783 -3.237024  4.787055  1.42857850  4.4556369</span>
<span class="co">#&gt; 16  1.34528408  3.085598 -3.618305 -1.55191817 -0.7393413</span>
<span class="co">#&gt; 17  0.69867536  2.571083 -4.509221 -0.90957855 -2.1490415</span>
<span class="co">#&gt; 18  2.32299459  3.652571 -4.530834 -0.26597688  1.1787542</span>
<span class="co">#&gt; 19  0.52138234  4.126736 -4.692299 -1.82869119 -1.8728723</span>
<span class="co">#&gt; 20  0.55470170  5.177442 -4.077158 -0.44885387  1.2061316</span>
<span class="co">#&gt; 21 -1.07783604 -4.738856  3.350785  0.43886354 -2.0270438</span>
<span class="co">#&gt; 22  1.56085040  4.311423 -2.607969  1.43984314  4.7041474</span>
<span class="co">#&gt; 23 -0.03108365 -4.440290  2.259249  1.63794946 -0.5741759</span>
<span class="co">#&gt; 24 -0.02875327 -5.547164  4.196997  1.83873855  0.4598187</span>
<span class="co">#&gt; 25  0.94184423  3.569038 -2.918003 -0.30211736  1.2907615</span>
<span class="co">#&gt; 26  1.62559235  3.422939 -2.279709  0.63659882  3.4054208</span>
<span class="co">#&gt; 27 -2.02812928 -4.033964  3.881897  1.19822699 -0.9819695</span>
<span class="co">#&gt; 28 -0.79208281 -5.581341  4.862226  2.30930244  0.7981047</span>
<span class="co">#&gt; 29 -0.39321914  4.333431 -4.981819  0.01214888 -1.0294588</span>
<span class="co">#&gt; 30 -0.04508029 -5.063118  5.824255  0.62219514  1.3382521</span>
<span class="co">#&gt; 31 -0.14193175  4.193722 -6.048656 -1.17981779 -3.1766837</span>
<span class="co">#&gt; 32 -0.84622041 -2.928197  2.890542  1.07135746  0.1874821</span>
<span class="co">#&gt; 33  0.76854650  5.138432 -5.152594 -0.13255323  0.6218314</span>
<span class="co">#&gt; 34  2.97458058  5.192781 -4.889379 -1.40013593  1.8778464</span>
<span class="co">#&gt; 35 -1.36202755 -4.522044  2.435318  1.56489719 -1.8838564</span>
<span class="co">#&gt; 36 -0.12343193 -1.768180  5.588877  1.58243381  5.2796990</span>
<span class="co">#&gt; 37 -1.23448220 -5.659886  2.223537  1.50447115 -3.1663604</span>
<span class="co">#&gt; 38  1.77231012  4.726546 -2.452772 -0.87475608  3.1713284</span>
<span class="co">#&gt; 39  1.36924806  4.116374 -5.812461 -0.40007091 -0.7269096</span>
<span class="co">#&gt; 40 -0.59555521 -4.775511  3.764182  2.90738062  1.3004964</span>
<span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Variance of row sums for ex_3 is "</span>, <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">ex_3</span><span class="op">$</span><span class="va">ROWSUM</span><span class="op">)</span>, <span class="st">"."</span>, sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span>
<span class="co">#&gt; Variance of row sums for ex_3 is 5.83295.</span></code></pre></div>
<p>Recall that we want to exploit exchangeability to detect the existence of statistically meaningful populations. The examples above show that exchangeability alone is insufficient for detecting hidden subpopulations from a finite sample. However, observe that a key difference between Examples 2 and 3 and Example 1 lies in feature dependencies. In Examples 2 and 3, the features are <em>not independent but conditionally independent</em>, where conditioning is done on the latent distribution membership. This suggests that feature dependency plays a role in <em>aligning</em> non-exchangeability with the existence of hidden subpopulations.</p>
</div>
<div id="tension-between-sample-exchangeability-and-feature-independence" class="section level2">
<h2 class="hasAnchor">
<a href="#tension-between-sample-exchangeability-and-feature-independence" class="anchor"></a>Tension between Sample Exchangeability and Feature Independence</h2>
<p>Consider Examples 1 and 2. To set the stage, suppose we have samples drawn from these two examples. Assume we have neither information about the generative model nor information about the observation labels (i.e., the subpopulation labels are latent). However, we know the samples were drawn from some population at random. With this information let us use <strong>flinty</strong> to detect exchangeability. We consider what happens when additional information is provided successively.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Load packages and set directories</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">flintyR</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">doParallel</span><span class="op">)</span>
<span class="co"># Register parallel computation</span>
<span class="fu"><a href="https://rdrr.io/pkg/doParallel/man/registerDoParallel.html">registerDoParallel</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p><strong>Scenario 1: We do not have information about feature dependencies.</strong> We might assume that the features are independent. This is correct for the array generated by Example 1, but not for Example 2.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Compute p-values </span>
<span class="fu"><a href="../reference/getPValue.html">getPValue</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">ex_array</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># Example 1 array</span>
<span class="co">#&gt; [1] 0.8004</span>
<span class="fu"><a href="../reference/getPValue.html">getPValue</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">ex_2</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co"># Example 2 array</span>
<span class="co">#&gt; [1] 0</span></code></pre></div>
<p>We see that at <span class="math inline">\(\alpha=0.05\)</span> we would consider Example 1‚Äôs array exchangeable, but not Example 2‚Äôs array. In other words, the array from Example 2 is not consistent with simultaneously being exchangeable and having independent columns. This demonstrates that treating features as independent when they are in fact not can lead to the appearance of sample non-exchangeability. Thus, sample exchangeability is <em>contingent on</em> accounting for feature dependencies.</p>
<p><strong>Scenario 2: We have information about feature dependencies.</strong> We know that the features in Example 1 are independent, so nothing changes from the previous scenario when running our test of exchangeability. However, for Example 2 we no longer run the independent features test, but run the dependent features test with all <span class="math inline">\(P=4\)</span> features grouped together as a block. This is because the features cannot be split into two or more independent sets of features.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## Compute p-value for Example 2 array again</span>
<span class="fu"><a href="../reference/getPValue.html">getPValue</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">ex_2</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, block_labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; All blocks are labeled 1, i.e., no independent sets of features detected, so samples are assumed exchangeable.</span>
<span class="co">#&gt; [1] 1</span></code></pre></div>
<p>Now we see that Example 2‚Äôs array is considered exchangeable.</p>
<p>By running <strong>flinty</strong> on our example datasets, we have revealed the tension between feature dependencies and sample exchangeability: a sample can appear non-exchangeable without us knowing if it is truly non-exchangeable or actually a consequence of feature dependencies. It is only with information about feature dependencies that we can have confidence in a claim about the sample being non-exchangeable.</p>
<p style="text-align:center;">
<img src="ex_vs_hom_visual.png" alt="Visualization of tension." width="800"></p>
<p align="center">
<em>Bruno sees how the accurate detection of non-exchangeability depends upon knowledge about the statistical dependencies between features, if not the generative mechanism itself.</em>
</p>
<details><summary><b>Is there a mathematical basis for this?</b>
</summary>
Yes there is, and Bruno‚Äôs musings have hinted at it. The tension we‚Äôve described is related to one of the fundamental problems in the analysis of multivariate data: given a <span class="math inline">\(N\times P\)</span> dataset <span class="math inline">\(\mathbf{X}\)</span>, how can we tease apart information that is shared between feature correlations and sample correlations? More concretely, to learn something about the samples we require some information or assumptions about the features, and vice-versa.  How is information shared between feature correlations and sample correlations? This simple fact comes from a basic result in linear algebra: the trace of a matrix is equal to the trace of its transpose. Starting with the dataset <span class="math inline">\(\mathbf{X}\)</span>, suppose we doubly standardize it; this means we transform it so that we obtain a version <span class="math inline">\(\overset{\circ}{\mathbf{X}}\)</span> for which the columns and rows of <span class="math inline">\(\overset{\circ}{\mathbf{X}}\)</span> have zero mean and unit variance. Then, <span class="math inline">\(\overset{\circ}{\mathbf{X}}\overset{\circ}{\mathbf{X}}^T\)</span> and <span class="math inline">\(\overset{\circ}{\mathbf{X}}^T\overset{\circ}{\mathbf{X}}\)</span> record the sample and feature covariances respectively. Yet these two covariances share the same eigenvalues, up to inclusion of additional eigenvalues with value <span class="math inline">\(0\)</span>. In particular, the sums of eigenvalues, which are the traces, are equal: <span class="math inline">\(\text{tr}\left(\overset{\circ}{\mathbf{X}}\overset{\circ}{\mathbf{X}}^T\right)=\text{tr}\left(\overset{\circ}{\mathbf{X}}^T\overset{\circ}{\mathbf{X}}\right)\)</span>. This phenomenon is clearly explained and also explored in the context of testing for sample independence by a <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-3/issue-3/Are-a-set-of-microarrays-independent-of-each-other/10.1214/09-AOAS236.full">2009 paper</a> of Efron.<br>
More broadly, a dataset <span class="math inline">\(\mathbf{X}\)</span> could arise from a multitude of generative processes: one in which observations come from multiple distinct generative processes, each with their own patterns of feature dependencies (sample and feature non-independence), one in which observations come from multiple generative processes that share the same partition of feature dependencies (sample non-independence but shared dependency structure), or one in which observations come from the same generative process (sample independence and shared dependency structure). The first scenario leads to <a href="https://doi.org/10.1111/j.1467-9868.2011.01027.x">transposable arrays</a>, a topic regarding which we leave the reader to pursue at their own interest.
</details>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Alan Aw.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
