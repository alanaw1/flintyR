---
title: "Exchangeability and Homogeneity"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exchangeable_Homogeneous}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

How many ways can a multivariate dataset be heterogeneous? What has exchangeability got to do with it? 

ðŸ“– In this vignette, we will

- explore different types of heterogeneities, connecting them to a "working definition" of *statistical homogeneity*, 
- explain how "statistical homogeneity" is better replaced by exchangeability (when trying to detect subpopulations in a given sample)
- investigate, using **flinty**, the tension between sample exchangeability and feature independence

The punchline is that non-exchangeability is a mathematically concrete way to capture our intuition of existence of subpopulations in a given sample, and there are types of statistical heterogeneities that still give rise to exchangeable data. Thus, users of our test should take care in interpreting results obtained from our test, or any other test of structured-ness of data (e.g., the [Tracy-Widom test](https://rdrr.io/bioc/LEA/man/main_tracyWidom.html) of largest singular value).      

# A "Working Definition" of Statistical Homogeneity

Scientists frequently think about homogeneous populations, homogeneous proportions, and homogeneous samples. We run tests of homogeneity on labeled samples (e.g., the $\chi^2$ test). With labeled samples, one could directly formalize homogeneity as "little to no difference with respect to labels." Indeed, the $\chi^2$ test of homogeneity is a test of whether categorical proportions are similar enough between distinctly labeled samples. So are many other tests of homogeneity defined (e.g., [Cochran's test](https://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/cochvari.htm) for homogeneity of variances between $k$ groups, the [Potthoff-Whittinghill test](https://onlinelibrary.wiley.com/doi/10.1002/9781118445112.stat01756) of homogeneity between two classes). The general test of homogeneity is typically a "test of homogeneity of quantity $x$ with respect to various groups of samples."   

Defining homogeneity without sample labels is less straightforward. Here are some approaches taken by others:

- In Volume 5 of the *Encyclopedia of Statistical Sciences* (second edition [published](https://www.wiley.com/en-gb/Encyclopedia+of+Statistical+Sciences%2C+16+Volume+Set%2C+2nd+Edition-p-9780471150442) in 2006 by Wiley), on p. 3207 the entry "Homogeneity and Tests of Homogeneity" begins with the sentence, "homogeneity refers to sameness or similarity." The rest of the paragraph, and indeed the remainder of the entry, frames homogeneity with respect to a collection of populations. 
- The environmental health book *Radiation-Induced Processes of Adaption: Research by Statistical Modeling* ([published](https://www.springer.com/gp/book/9789400766297) in 2013 by Springer) writes in their Glossary, on p. 173, that homogeneity "relate[s] to the validity of the often convenient assumption that the statistical properties of any one part of an overall dataset are the same as any other part."   

The statistics encyclopedia does not provide any definition of homogeneity of a dataset without labels. This reflects our point in the beginning about how many tests of homogeneity typically require sample labels. On the other hand, the environmental health book describes a formalizable description of homogeneity, namely the clause about statistical properties of any one part looking the same as another. 

Mathematically, the description above says that given a dataset $\{\mathbf{x}_1,\ldots,\mathbf{x}_N\} \subset \mathbb{R}^P$, for it to be homogeneous it should satisfy the following property: for any pair of disjoint subsamples $\mathcal{S}=\{\mathbf{x}_i\}$ and $\mathcal{S}'=\{\mathbf{x}_{i'}\}$, the distributions of the pair of subsamples $F_\mathcal{S}$ and $F_{\mathcal{S}'}$ should not differ by too much: 
\begin{equation}
d(F_\mathcal{S},F_{\mathcal{S}'}) \approx 0,\label{eq:1}
\end{equation}
where $d(\cdot,\cdot)$ is some measure of distance between distributions (e.g., Wasserstein distance or total variation distance).

Suppose we adopt the description above as a working definition of statistical homogeneity. We shall explain why this definition is problematic. 

## A Single Population Doesn't Imply "Statistical Homogeneity"

Let us define a population, for which there are $P$ observable features. These features are statistically independent and identically distributed according to the mixture distribution $\frac{1}{2}N(-2,1) + \frac{1}{2}N(2,1)$. This means that we flip a fair coin to decide whether a feature is drawn from $N(-2,1)$ or from $N(2,1)$.    

For illustrative purposes, let's assume that $P=2$ and draw $N=40$ samples from this distribution. 

```{r example, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
## Helper function to draw samples
drawSamples <- function(N)  {
  # create sequence to return
  to_return <- c()
  
  # grow the sequence
  for (n in 1:N) {
    # flip a fair coin
    p <- rbinom(1,1,0.5) 
    
    # if heads...
    if (p == 1) {
      to_return <- c(to_return, rnorm(1, mean = 2, sd = 1)) 
    } else {
      # if tails...
      to_return <- c(to_return, rnorm(1, mean = -2, sd = 1))
    }
  }
  
  # return
  return(to_return)
}

## Generate array
set.seed(2021)
ex_array <- replicate(2, drawSamples(40))

## Annotate positive and negative labels
library(tidyverse)
ex_array <- as.data.frame(ex_array)
ex_array$POSLAB <- (-1 * (ex_array[,1] < 0 & ex_array[,2] < 0) +
                      (ex_array[,1] > 0 & ex_array[,2] > 0)) 

ex_array <- ex_array[order(ex_array$POSLAB),]

## Print
print(ex_array)
```
Notice we have added positive labels in the third column to annotate whether both features are positive or negative. The example shows that this sample forms two clusters of negative and positive observation pairs, $\mathcal{S}_-$ and $\mathcal{S}_+$. Clearly these disjoint subsamples are far apart, thus rendering the dataset itself not statistically homogeneous under the working definition. See the 2D plot below for a visualization.

```{r example_2, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
## Plot 2D 
ratio <- with(ex_array, diff(range(V1))/diff(range(V2))) # compute ratio to fix make plot square

ggplot(data = ex_array, aes(x = V1, y = V2)) +
  geom_point(aes(colour = factor(POSLAB))) +
  coord_fixed(ratio = ratio) +
  theme_bw() +
  labs(colour = "POSLAB")
```

This observation is true in a more general setting. For $P$ features independently distributed according to the mixture distribution described above, it is always possible to draw a finite sample consisting of $N$ observations, which contains two disjoint subsamples that are far apart. Here is some mathematical intuition. For each observation $\mathbf{x}_n$, let $\mathbf{1}\{\mathbf{x}_n > \mathbf{0}\}$ and $\mathbf{1}\{\mathbf{x}_n < \mathbf{0}\}$ denote, respectively, the event that it is positive and the event that it is negative. A positive observation $\mathbf{x}_n=(x_{n1},\ldots,x_{nP})$ means that each element is positive; likewise for a negative observation. Each component of $\mathbf{x}_n$ is IID according to the mixture distribution that is symmetric about $0$. This implies that each element $x_{nj}$ satisfies $\mathbb{P}(x_{nj}>0) =\mathbb{P}(x_{nj}<0) = 0.5$. Multiplying these probabilities, we see that $\mathbb{P}(\mathbf{x}_n > \mathbf{0})=\mathbb{P}(\mathbf{x}_n < \mathbf{0})=0.5^P$. The indicators are thus Bernoulli distributed with success probability $0.5^P$. Let $K_+$ and $K_-$ denote the number of positive and negative observations contained in a random draw. Since our observations are independently drawn, we see that for any pair of natural numbers $(k_+,k_-)$,
\begin{equation*}
\mathbb{P}(K_+=k_+, K_-=k_-) = {N\choose k_+}{N-k_+ \choose k_-} \left(\frac{1}{2^P}\right)^{k_+ + k_-}.
\end{equation*}
In particular, there is a positive probability that the finite sample drawn at random contains two sets $\mathcal{S}_+, \mathcal{S}_-$ of sizes $k_+$ and $k_-$. These two sets containing positive observations and negative observations exclusively, we see that they are necessarily far apart. 

In fact, it can be shown that for large enough sample sizes, with high probability a finite sample drawn will contain two reasonably large disjoint subsamples $\mathcal{S}_+$ and $\mathcal{S}_-$ that are far apart (i.e., with total variation distance $d_{\text{TV}}(F_{\mathcal{S}_+},F_{\mathcal{S}_-}) =1$).

<details>
  <summary><b>I like maths. Show me the mathematical details.</b></summary>
     Observe that $K_+$ and $K_-$, defined above, can be written as $K_+=\sum_{n=1}^N \mathbf{1}\{\mathbf{x}_n > \mathbf{0}\}$ and $K_-=\sum_{n=1}^N \mathbf{1}\{\mathbf{x}_n < \mathbf{0}\}$. We already saw that the indicator events $\mathbf{1}\{\cdot\}$ are Bernoulli distributed with success probability $0.5^P$. Thus $K_+$ and $K_-$ are both $\text{Bin}(N,0.5^P)$ distributed.<br/> 
     Bernoulli distributions are subGaussian with subGaussian parameter $\sigma^2=0.25$, regardless of the success probability $p$. Thus $\text{Bin}(N,p)$ distributions are subGaussian with subGaussian parameter $\sigma^2=0.25N$. Being subGaussian implies that a distribution has nice tail bounds. More precisely, Chernoff's bound implies that $K_+$ and $K_-$ satisfy the following concentration inequalities: for any positive $\epsilon$,
\begin{eqnarray*}
\mathbb{P}(|K_+ - \mathbb{E}[K_+]| > \epsilon) & \leqslant & 2\exp\left(-\frac{2\epsilon^2}{N}\right), \\
\mathbb{P}(|K_- - \mathbb{E}[K_-]| > \epsilon) & \leqslant & 2\exp\left(-\frac{2\epsilon^2}{N}\right).
\end{eqnarray*}
Let's use these concentration inequalities to show that for a finite sample drawn, $K_+$ and $K_-$ will be close to their expectations with high probability. Then as $N$ becomes large enough, we will see that $K_+$ and $K_-$ become reasonably large.<br/> 
     First, by linearity of expectation we have 
\begin{equation*}
\mathbb{E}[K_+] = \mathbb{E}[K_-] = \frac{N}{2^P}.
\end{equation*}
     Next, by the union bound 
\begin{eqnarray*}
\mathbb{P}(|K_+ - \mathbb{E}[K_+]| > \epsilon \vee |K_- - \mathbb{E}[K_-]| > \epsilon) & \leqslant & \mathbb{P}(|K_+ - \mathbb{E}[K_+]| > \epsilon) + \mathbb{P}(|K_- - \mathbb{E}[K_-]| > \epsilon) \\
& \leqslant & 4 \exp\left(-\frac{2\epsilon^2}{N}\right),
\end{eqnarray*}
which implies that the complementary event (the "good" event) satisfies
\begin{equation*}
\mathbb{P}(|K_+ - \mathbb{E}[K_+]| \leqslant \epsilon \wedge |K_- - \mathbb{E}[K_-]| \leqslant \epsilon) \geqslant 1 - 4 \exp\left(-\frac{2\epsilon^2}{N}\right).
\end{equation*}
Now set $\epsilon = N^{0.55}$ and let $N=M \cdot 2^P$ (so that $N$ is some multiple $M$ of $2^P$ where we shall choose $M$ large enough). The last inequality says that with probability at least $1 - 4\exp\left(-2N^{0.1}\right)=1-4\exp\left(-2 \cdot 2^{0.1P} \cdot M^{0.1}\right)$, a random sample contains $K_+$ and $K_-$ positive and negative observations, with
\begin{equation*}
M - 2^{0.55P}\cdot M^{0.55} \leqslant K_+, K_- \leqslant M + 2^{0.55P}\cdot M^{0.55}.
\end{equation*}
This shows that $K_+$ and $K_-$ grow like $M$ with probability $1-4\exp\left(-2 \cdot 2^{0.1P} \cdot M^{0.1}\right)$, a quantity that approaches $1$ as $M\to\infty$. Thus, given fixed $P$, for $M$ large enough (so that the sample size $N$ is also large), we have two reasonably large disjoint subsamples $\mathcal{S}_+$ and $\mathcal{S}_-$ that are far apart. <br/>
     For perspective, set $P=5$ and $M=100$. We draw $N=M\times 2^P = 3200$ observations at random from the population. The argument above says that with probability at least $1-4 \exp\left(-2 \cdot 2^{0.1P} \cdot M^{0.1}\right) = 0.95$, the sample will have $K_+$ and $K_-$ positive and negative samples, with $K_+$ and $K_-$ lying between $M - 2^{0.55P}\cdot M^{0.55}\approx 15.3$ and $M - 2^{0.55P}\cdot M^{0.55}\approx 184.7$.<br/>  
     To finish the argument, we have to quantify "far apart" in terms of some distance computed between the two empirical distributions obtained from the positive and negative sets, $\mathcal{S}_+$ and $\mathcal{S}_-$. We shall use the total variation distance, 
\begin{equation*}
d_\text{TV}(F,F') = \sup\{|\mu_F(A) - \mu_{F'}(A)|: A \subseteq \mathbb{R}^P\}.
\end{equation*}
     The total variation distance is the largest possible difference between the probabilities that the two probability distributions $\mu_F$ and $\mu_F'$ can assign to the same event. It also ranges between $0$ and $1$. Now for $\mathcal{S}_+$ and $\mathcal{S}_-$, the corresponding empirical distributions, denoted by $\mu_+$ and $\mu_-$, are fully supported on $\mathbb{R}_+^P$ and $\mathbb{R}_-^P$ respectively. Thus the largest possible difference is at least the difference between the probabilities that $\mu_+$ and $\mu_-$ assign to the set of positive $P$-dimensional reals, which is $1$. Therefore $d_{\text{TV}}(F_{\mathcal{S}_+},F_{\mathcal{S}_-}) \geqslant 1$, which explicitly quantifies that the subsamples are distributionally far apart. (In fact it is equal to $1$, since $d_\text{TV}$ cannot exceed $1$ as was mentioned earlier.)   
</details> 

The example above and its generalization show that even if our data originates from a single population, statistical heterogeneity may still arise in finite samples. It also points to the need for a careful solution for deciding whether a finite sample originated from a single population. 

We will see how exchangeability provides such a solution. To motivate our discussion about exchangeability later, let us first see a few examples of heterogeneous samples.    

## Examples of Heterogeneity

1. Mixture Distributions and Independent Features 
2. Mixture Distributions and Conditionally Independent Features (I)
3. Mixture Distributions and Conditionally Independent Features (II)

# Exchangeability 

To be written.

## Tension between Sample Exchangeability and Feature Independence

